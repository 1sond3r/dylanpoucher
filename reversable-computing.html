<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-05-30 Fri 21:02 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Reversable Computing</title>
<meta name="author" content="Dylan Poucher" />
<meta name="generator" content="Org Mode" />
<style type="text/css">
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="style.css"/><link href="https://fonts.googleapis.com/css2?family=Ibarra+Real+Nova&display=swap" rel="stylesheet">
<div class="navbar"><a href="index.html">Home</a> | <a href="about.html">About</a></div>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Reversable Computing
<br />
<span class="subtitle">How to save the universe</span>
</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org82f0c00">1. Intro</a></li>
<li><a href="#org6baafec">2. The Idea</a></li>
<li><a href="#orgd96d8bc">3. Fuck</a></li>
<li><a href="#org88fd4a9">4. The Idea</a></li>
</ul>
</div>
</div>
<div id="outline-container-org82f0c00" class="outline-2">
<h2 id="org82f0c00"><span class="section-number-2">1.</span> Intro</h2>
<div class="outline-text-2" id="text-1">
<p>
I was reminded of an interesting concept a few days ago called reversible
computation. I think I first heard about it years ago when I was going though a
period of enormous concern about the heat death of the universe: the prospect
that in the far far future entropy throughout the universe will have increased
to such an extent that there are no temperature gradients, thus no work, thus no
computation, and since computation is presumably necessary for conscious life
there is no possibility of anyone or anything being around to comment on the
dark black inky blankness.
</p>

<p>
Lots of other people have worried about this too - no less Freeman Dyson, a man
who pops up at alarming frequency throughout the zanier ends of science. He
proposed in the 70s that a simulated race of post humans could actually extend
subjective time infinitely whilst only expending a finite amount of energy
through reversible computation. I caught that reversibility had something to do
with the possibility of input to be entirely reconstructed from the output, and
that this lead to improved efficiency, but my 15 year old mind didn't care so
much about the details and was much set at rest by this clever man saying that
things were going to be fine.
</p>

<p>
Well this phrase "reversible computing" continued to knock around in my head for
like five years until I eventually regurgitated it to a friend in a discussion
about something or other, which led me to actually clarify my understanding a
bit. This article will hopefully serve as a good tool in recapitulating that
understanding! My presentation style is going to be innovative and stream of
consciousness so its like you're really coming along with me for the ride!
</p>
</div>
</div>
<div id="outline-container-org6baafec" class="outline-2">
<h2 id="org6baafec"><span class="section-number-2">2.</span> The Idea</h2>
<div class="outline-text-2" id="text-2">
<p>
Classical computation is not information-preserving. Obviously, a typical
computer clears registers to make space for more information, etc. In fact, any
system which can be represented by a many-to-one function does not preserve
information - and the nature of typical logic gates are many-to-one. In an OR
gate three of the four unique inputs are assigned a high output, given knowledge
just of the high output we could not know which input it corresponded to. We can
see then how this information loss makes the computation irreversible, and the
irreversibly is enormously compounded by combining these gates together, say to
make an adder. On an abstract level, we can see how simple two figure addition
is already many to one: given the output "8" I can already permute "1 + 7", "2 +
6", "3 + 5", etc as possible inputs.
</p>

<p>
The concept of information preservation is not so abstruse as it sounds: we just
need to make our functions one-to-one. To do this we simply need to map inputs
to themes plus the result of the operation. For example, the output of "3 + 5"
becomes "3 + 5 AND 8." This preserves a lovely one-to-one mapping and hence
reversibility. If we extended this method to the level of gates themselves we
already have the beginnings of a fully reversible computing machine! To labour
the OR gate example: &lt;1, 1&gt; -&gt; &lt;&lt;1,1&gt; 1&gt;, &lt;1, 0&gt; -&gt; &lt;&lt;1,0&gt; 1&gt;, etc. Although,
the gates which tend to be used in reversible logic tend to be quite different
to this.
</p>

<p>
Why is this important? Well, this links back to information. As with all
abstract concepts there appear to be a million different interpretations of a
million different formalisations each with their own deep history and
applications. However, in the sense important for reversible computing,
information is something to do with entropy. Entropy is colloquially labelled as
"disorder" and it kind of is, but a lot of things which seem quite ordered to us
are actually high entropy, and vice-versa. The heat death of the universe for
instance is high-entropy despite it not seeming to have a lot going on, and the
start of the universe is low entropy despite it being a big mess.
</p>

<p>
Entropy, and by extension information, turn out to be pretty subjective
construct relating to how models of systems succeed in representing systems.
Models of systems are said to be "course grained" if they sacrifice description
of individual elements of a system in return for the general behaviour of a
system. This all links back to the evil of statistical mechanics (which I
suppose in some sense itself is a course grained model, whoops). Ideal gas laws
are a good example. There is a lot of shit going on in a gas of uniform
pressure, temperature, etc: probably at least 14 zillion little guys bouncing
around pretty unpredictably. Yet, the ideal gas laws are amazingly simple: PV =
nRT. This description is extremely course-grained, yet it still gives us very
accurate information about the behaviour o f gas under certain conditions. How
many conditions? The individual 14 zillion molecules can be arranged in 323
zingobadrillion different ways and still produce a system accurately described
by our course-grained model, it doesn't really matter if that particular
molecule there is helium or argon - it is the number of ways a system can be
arranged and still conforms to a model that leads to entropy. It is said that
high entropy systems have many microstates corresponding to one macro state.
</p>

<p>
Contrast the ideal gas scenario to the famously bitchy naiver-stokes equations
describing the flow of a viscous fluid: hey
are just horrendous, horrific, awful and famously shitty to solve. This in some
sense is because viscous fluid flow is a highly structured system: I.E low
entropy. It is highly chaotic: small changes in the positioning of a
(relatively) small number of molecules can enormously change the behaviour of
the overall system. Thus: a finer-grained model is necessary to capture the
behaviour: making it such that fewer microstates that correspond to a
macrostate. Notice here though that the "macrostate" is defined by the model,
and the model is created by humans for some purpose. Entropy increase refers to
a situation where we can acceptably model the changing macrostate in such a way
that the number of microstates corresponding to it increases. I found this
really a revelation and it has certain implications on the flow of time and
other crazy shit. Maybe time is the result of our models of the world changing
in order to more efficiently facilitate predictions : the "easiness" and thus
direction of which is arbitrary. This is a Richenbachean view and I don't know
if its right but its sure as hell hard to prove wrong on his own terms.
</p>
</div>
</div>
<div id="outline-container-orgd96d8bc" class="outline-2">
<h2 id="orgd96d8bc"><span class="section-number-2">3.</span> Fuck</h2>
<div class="outline-text-2" id="text-3">
<p>
Basically though, entropy tends to go up in a closed system: our models tend to
become simpler. This is bad news for life though because models becoming simpler
eventually means that nothing complex happens like life and consciousness and
other cool shit! This goes for any system we want to run forever: entropy needs
to not increase. Thus, if we want a computer to run forever, it needs to operate
in a closed system which entropy does not increase: I.E: the system behaves
exactly like a model. How the fuck do we do that? By preserving all information
within the system! Information here then becomes a sort of interface between
models of different granularity, information "loss" is the disparity of
behaviour between higher and lower grained models. We need to create a
computational system which is so well understood and modelled that there is no
disparity between it and reality (I.E: a perfect model of the universe).
Information erasure within a model of computing necessitates a disparity between
the model of computation and reality: where the information "goes" is
unaccounted for and sorted out by some other physical models. Thus, for a model
of computation to have zero entropy increase it necessarily must preserve
information and hence be reversible. This is all summed up in Landauer’s
Principle which basically stipulates there will always be an energy cost
associated with erasing bits but not necessarily in propagating them.
</p>

<p>
Even if we make a computer with reversible logic there are still lots of things
increasing entropy: resistance in conductors, etc. Using reversible logic
usually increases the overhead of actually making the damn thing do anything
useful (not being able to erase bits makes memory management much more
complicated), so the tradeoff isn't yet worth it. However, if the energy loss
per computation gets low enough such that the information loss actually becomes
a significant factor, reversible computing becomes viable. There is actually a
principal in thermodynamics that basically the slower you run a system the less
energy loss you are going to get. So, in theory, if we run our computers
arbitrarily slowly reversibility will actually become a factor (if ran slow
enough, even the factor) in further improving efficiency. Hense mister Dyson's
suggestion that we slow down the subjective time of our future simulations so
we can squeeze the most computation out of the universe as possible!
</p>

<p>
Interestingly enough, many kinds of quantum computers are also reversible in
nature! This maybe should be expected because entanglement entails a sort of
lack of information leakage from the entangled system. This isn't to say that
all forms of quantum computation are reversible, some have intermediate stages
where qbits are disentangled, etc. And of course the <span class="underline">output</span> of a quantum
computer must involve some kind of collapse. Even so, kind of interesting, and
quantum computers have been suggested as a promising route to making a
physically realisable mega efficient computing machine because of this
reversibility. So maybe our decedents will be using suitably futuristic quantum
shit made by Google or IBM or something.
</p>

<p>
Well that's kind of cool but what does a reversible computer look like?! How do
you program it? How do you use the logic gates? I will explain&#x2026;&#x2026;
</p>

<p>
Let me introduce you to some gates:he
input/output schema is given with boolean algebra notation which I didn't know
but its pretty simple. The big letters A, B, C, etc are just boolean variables
which take the value 0 or 1, pretty standard. When you see two together like AB
this is literally multiplication and so the logical operation is a conjunction:
"AND" (I.E 1x1 = 1, 1x0 = 0&#x2026; logically like a conjunction). "⊕" means "XOR",
"+" "OR", the apostrophe means "not" and there are some others probably but this
is all I have seen and its enough for everything.
</p>

<p>
Looking at these you can see how you can trace back the output of each gate to
the input. The Feynman gate for example is basically an XOR plus an extra output
(A itself) to keep track of which of A or B was high. The others are a bit more
complicated but in principal, as it tends to be with these sorts of things, can
all be built up from the Feynman gate.
</p>

<p>
With these you can make a full adder, like these Algerian gentleman did:
</p>






<p>
 (starting to see some similarities to irreversibility?!) Consideration of this
leads us to entropy. and is hence non-reversible. I was reminded of an
interesting concept a few days ago called reversible computation. I think I
first heard about it years ago when I was going though a period of enormous
concern about the heat death of the universe: the prospect that in the far far
future entropy throughout the universe will have increased to such an extent
that there are no temperature gradients, thus no work, thus no computation, and
since computation is presumably necessary for conscious life there is no
possibility of anyone or anything being around to comment on the dark black inky
blankness.
</p>

<p>
Lots of other people have worried about this too - no less Freeman Dyson, a man
who pops up at alarming frequency throughout the zanier ends of science. He
proposed in the 70s that a simulated race of post humans could actually extend
subjective time infinitely whilst only expending a finite amount of energy
through reversible computation. I caught that reversibility had something to do
with the possibility of input to be entirely reconstructed from the output, and
that this lead to improved efficiency, but my 15 year old mind didn't care so
much about the details and was much set at rest by this clever man saying that
things were going to be fine.
</p>

<p>
Well this phrase "reversible computing" continued to knock around in my head for
like five years until I eventually regurgitated it to a friend in a discussion
about something or other, which led me to actually clarify my understanding a
bit. This article will hopefully serve as a good tool in recapitulating that
understanding! My presentation style is going to be innovative and stream of
consciousness so its like you're really coming along with me for the ride!
</p>
</div>
</div>
<div id="outline-container-org88fd4a9" class="outline-2">
<h2 id="org88fd4a9"><span class="section-number-2">4.</span> The Idea</h2>
<div class="outline-text-2" id="text-4">
<p>
Classical computation is not information-preserving. Obviously, a typical
computer clears registers to make space for more information, etc. In fact, any
system which can be represented by a many-to-one function does not preserve
information - and the nature of typical logic gates are many-to-one. In an OR
gate three of the four unique inputs are assigned a high output, given knowledge
just of the high output we could not know which input it corresponded to. We can
see then how this information loss makes the computation irreversible, and the
irreversibly is enormously compounded by combining these gates together, say to
make an adder. On an abstract level, we can see how simple two figure addition
is already many to one: given the output "8" I can already permute "1 + 7", "2 +
6", "3 + 5", etc as possible inputs.
</p>

<p>
The concept of information preservation is not so abstruse as it sounds: we just
need to make our functions one-to-one. To do this we simply need to map inputs
to themes plus the result of the operation. For example, the output of "3 + 5"
becomes "3 + 5 AND 8." This preserves a lovely one-to-one mapping and hence
reversibility. If we extended this method to the level of gates themselves we
already have the beginnings of a fully reversible computing machine! To labour
the OR gate example: &lt;1, 1&gt; -&gt; &lt;&lt;1,1&gt; 1&gt;, &lt;1, 0&gt; -&gt; &lt;&lt;1,0&gt; 1&gt;, etc. Although,
the gates which tend to be used in reversible logic tend to be quite different
to this.
</p>

<p>
Why is this important? Well, this links back to information. As with all
abstract concepts there appear to be a million different interpretations of a
million different formalisations each with their own deep history and
applications. However, in the sense important for reversible computing,
information is something to do with entropy. Entropy is colloquially labelled as
"disorder" and it kind of is, but a lot of things which seem quite ordered to us
are actually high entropy, and vice-versa. The heat death of the universe for
instance is high-entropy despite it not seeming to have a lot going on, and the
start of the universe is low entropy despite it being a big mess.
</p>

<p>
Entropy, and by extension information, turn out to be pretty subjective
construct relating to how models of systems succeed in representing systems.
Models of systems are said to be "course grained" if they sacrifice description
of individual elements of a system in return for the general behaviour of a
system. This all links back to the evil of statistical mechanics (which I
suppose in some sense itself is a course grained model, whoops). Ideal gas laws
are a good example. There is a lot of shit going on in a gas of uniform
pressure, temperature, etc: probably at least 14 zillion little guys bouncing
around pretty unpredictably. Yet, the ideal gas laws are amazingly simple: PV =
nRT. This description is extremely course-grained, yet it still gives us very
accurate information about the behaviour o f gas under certain conditions. How
many conditions? The individual 14 zillion molecules can be arranged in 323
zingobadrillion different ways and still produce a system accurately described
by our course-grained model, it doesn't really matter if that particular
molecule there is helium or argon - it is the number of ways a system can be
arranged and still conforms to a model that leads to entropy. It is said that
high entropy systems have many microstates corresponding to one macro state.
</p>

<p>
Contrast the ideal gas scenario to the famously bitchy naiver-stokes equations
describing the flow of a viscous fluid:hey
are just horrendous, horrific, awful and famously shitty to solve. This in some
sense is because viscous fluid flow is a highly structured system: I.E low
entropy. It is highly chaotic: small changes in the positioning of a
(relatively) small number of molecules can enormously change the behaviour of
the overall system. Thus: a finer-grained model is necessary to capture the
behaviour: making it such that fewer microstates that correspond to a
macrostate. Notice here though that the "macrostate" is defined by the model,
and the model is created by humans for some purpose. Entropy increase refers to
a situation where we can acceptably model the changing macrostate in such a way
that the number of microstates corresponding to it increases. I found this
really a revelation and it has certain implications on the flow of time and
other crazy shit. Maybe time is the result of our models of the world changing
in order to more efficiently facilitate predictions : the "easiness" and thus
direction of which is arbitrary. This is a Richenbachean view and I don't know
if its right but its sure as hell hard to prove wrong on his own terms.
</p>

<p>
Basically though, entropy tends to go up in a closed system: our models tend to
become simpler. This is bad news for life though because models becoming simpler
eventually means that nothing complex happens like life and consciousness and
other cool shit! This goes for any system we want to run forever: entropy needs
to not increase. Thus, if we want a computer to run forever, it needs to operate
in a closed system which entropy does not increase: I.E: the system behaves
exactly like a model. How the fuck do we do that? By preserving all information
within the system! Information here then becomes a sort of interface between
models of different granularity, information "loss" is the disparity of
behaviour between higher and lower grained models. We need to create a
computational system which is so well understood and modelled that there is no
disparity between it and reality (I.E: a perfect model of the universe).
Information erasure within a model of computing necessitates a disparity between
the model of computation and reality: where the information "goes" is
unaccounted for and sorted out by some other physical models. Thus, for a model
of computation to have zero entropy increase it necessarily must preserve
information and hence be reversible. This is all summed up in Landauer’s
Principle which basically stipulates there will always be an energy cost
associated with erasing bits but not necessarily in propagating them.
</p>

<p>
Even if we make a computer with reversible logic there are still lots of things
increasing entropy: resistance in conductors, etc. Using reversible logic
usually increases the overhead of actually making the damn thing do anything
useful (not being able to erase bits makes memory management much more
complicated), so the tradeoff isn't yet worth it. However, if the energy loss
per computation gets low enough such that the information loss actually becomes
a significant factor, reversible computing becomes viable. There is actually a
principal in thermodynamics that basically the slower you run a system the less
energy loss you are going to get. So, in theory, if we run our computers
arbitrarily slowly reversibility will actually become a factor (if ran slow
enough, even the factor) in further improving efficiency. Hense mister Dyson's
suggestion that we slow down the subjective time of our future simulations so
we can squeeze the most computation out of the universe as possible!
</p>

<p>
Interestingly enough, many kinds of quantum computers are also reversible in
nature! This maybe should be expected because entanglement entails a sort of
lack of information leakage from the entangled system. This isn't to say that
all forms of quantum computation are reversible, some have intermediate stages
where qbits are disentangled, etc. And of course the <span class="underline">output</span> of a quantum
computer must involve some kind of collapse. Even so, kind of interesting, and
quantum computers have been suggested as a promising route to making a
physically realisable mega efficient computing machine because of this
reversibility. So maybe our decedents will be using suitably futuristic quantum
shit made by Google or IBM or something.
</p>

<p>
Well that's kind of cool but what does a reversible computer look like?! How do
you program it? How do you use the logic gates? I will explain&#x2026;&#x2026;
</p>

<p>
Let me introduce you to some gates: he
input/output schema is given with boolean algebra notation which I didn't know
but its pretty simple. The big letters A, B, C, etc are just boolean variables
which take the value 0 or 1, pretty standard. When you see two together like AB
this is literally multiplication and so the logical operation is a conjunction:
"AND" (I.E 1x1 = 1, 1x0 = 0&#x2026; logically like a conjunction). "⊕" means "XOR",
"+" "OR", the apostrophe means "not" and there are some others probably but this
is all I have seen and its enough for everything.
</p>

<p>
Looking at these you can see how you can trace back the output of each gate to
the input. The Feynman gate for example is basically an XOR plus an extra output
(A itself) to keep track of which of A or B was high. The others are a bit more
complicated but in principal, as it tends to be with these sorts of things, can
all be built up from the Feynman gate.
</p>

<p>
With these you can make a full adder, like these Algerian gentleman did:
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Dylan Poucher (<a href="mailto:dylanp@dylan-20khs0cv00">dylanp@dylan-20khs0cv00</a>)</p>
<p class="date">Date: 20-12-25</p>
<p class="creator"><a href="https://www.gnu.org/software/emacs/">Emacs</a> 30.1 (<a href="https://orgmode.org">Org</a> mode 9.7.11)</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
