yyyyyyy
#+TITLE: David Bowie's "Young Americans"
#+SUBTITLE: His most important album
#+TAGS: Culture
#+DATE: 29-07-25
#+CONVERT: yes
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="style.css"/>
#+HTML_HEAD: <div class="navbar"><a href="index.html">Home</a> | <a href="about.html">About</a></div> <div class ="box"></box>

#+OPTIONS: html-postamble:<p>Published on <span class="post-footer-date">DATE_HERE</span> by <span class="post-footer-name">Dylan</span></p>
#+OPTIONS: toc:nil
#+OPTIONS: num:nil
#+HTML_HEAD_EXTRA: <style>  img { max-width: 100%; height: auto; } </style>

* Explaining Zollman
It begins like this:

Every round, each agent runs a series of experiments. The goal is to see if the theory B is better than theory A. Theory here is a slight misnomer, think of them more like techniques - like methods of surgery. In performing an experiment, an agent is simultaneously relying on it while also testing its efficacy. In this sense, the agents are not pure truth seekers but rather utility maximisers in an environment where we assume utility corresponds with truth. This is the Bala-Goyal model.

The agents determine the expected value using the mean of the beta-binomial distribution. This has some technicalities surrounding it, which we'll utilise later, but the mean for now can be described quite intuitively.

If we tossed some object like a weighted die 100 times, and 40 out of those 100 times it landed on 6 - what probability should we rationally ascribe to it landing 6 again? It appears it should just be the total number of occurrent 6s divided by the total number of trials. I.E, 40/100, which is equal to 0.4. The same logic applies to the beta-binomeal mean: for each agent, the expected value of each theory $T$ is equal to $\hat{\mu}_T = \frac{\alpha_T}{\alpha_T + \beta_T}$. Where $\alpha_T$ is the total number of successes produced by a theory, and $\beta_T$ the total number of failures elicited by the same theory. Well, to be more accurate, to prevent a division by zero at the beginning of the simulation, we randomly assign pre-experimental alpha and beta priors to all agents. These act as "phantom trials."

Technique A has a success rate known to the agents (0.5), while technique B has a success rate they must determine (0.6). We model our agents as "greedy." That is, they test the theory they determine to have the highest expected value based off past experiments. Agents don't merely see the results of their own experiments however: they also share their results with neighbours over a network. The topology of this network can vary in multitudinous and exciting ways - and is the principal object of study by Zollman in his paper "The Communication Structure of Epistemic Communities" (2007). His key finding, and what appears to be the key finding of the entire field, is that less connected networks counter-intuitively converge upon the best theory more regularly than highly connected networks.

Here are some examples of different network topologies (from Wikipedia):

#+CAPTION: A Complete graph. Every node is connected to every other node.
#+attr_html: :width 50%;
https://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Complete_graph_K5.svg/2304px-Complete_graph_K5.svg.png

Information spreads quickly in highly connected networks: if theory B fails for somebody, lots of people hear about it. Conversely, networks represented by graphs like the cycle have much less communication, each agent having much fewer neighbours.

#+CAPTION: A Cycle graph. Every node is connected to exactly two other nodes.
#+attr_html: :width 50%;
https://upload.wikimedia.org/wikipedia/commons/thumb/2/24/Circle_graph_C5.svg/1142px-Circle_graph_C5.svg.png

One might reasonably expect that more communication, and thus using complete graphs in our model, is effective for knowledge gathering. However, embed the logic we have been discussing in a python simulation and something rather interesting occurs:

#+attr_html: :width 100%;
[[file:///./Zollman_Complete_Fail.png ]]







This has advantages and disadvantages. If B is an obviously superior theory, all agents learn about its successes rapidly and switch from poor theories early. Conversely if B is only marginally better than the alternative


* Expanding to Multi-Armed Bandit
: Explain multi armed bandit and give example

We can expand this to a multi-armed bandit quite easily. We now just add more theories, each with different rates of success. We gather data on them in much the same way as we did with theory B, except, there is now no "given" theory. Each will just have the credence as calculated by the Beta binomial. Each agent simply uses the theory they have the most credence in.

This has some amusing effects. We occasionally get rapid changes between different theories, which sort of looks like a paradigm shift. This happens when a bad theory gets lucky at the start, and becomes ubiquitous. It may be, however, that one or more agent's next best theory is superior to this theory. If the ubiquitous theory proves itself to be inferior to the credence the agent has in that theory, they will switch. This starts broadcasting information to the rest of the community about the theory. Since it is in fact superior, it eventually creates a cascade. I have seen this happen up to three times in a run! It is almost Kuhnian!

** Risk takers and Risk Aversion
: Explain my own model, interpret the results

My model is a variation on the multi-armed bandit as described above. After some experimentation with different ideas (in group/out-group biasing, bounded rationality, etc) I found the results of this model the most interesting.

The idea sort of came from the "Division of Cognative Labour" idea. That is, the concept that it is epistemically advantageous for the community for different agents to have different priorities to eachother. There was a model relased in 2009, based on the "epistemic landscapes" modelling paradigm, which tried to show how a group of "mavericks" who deliberately avoided agents following nearby theories, added to a group of "followers" and "climbers", systematically improved group performance. Now, this specific result was refuted in 2015 because of a humiliating programming error, but I think the idea remains intriguing.

I want you to cast your mind back to the beta binomeal distribution. This distribution not only has a mean associated with it, but a varience, calculated by (with use of gamma distribution magic)

$$\hat{\mu}_T = \frac{\alpha}{\alpha + \beta}$$

$$ \sigma^{2}_{T} = \frac{\alpha\beta}{(\alpha+\beta+1)(\alpha+\beta)^2}$$

I take the varience in this model to correlate to something along the lines of "the uncertainty in my confidence of this theory." It seems intuitive, that within a given community of scientists, there are going to be differences of temperament when it comes to this kind of uncertainty. Some - and I include myself in this category - will be averse to using a theory with a high varience. After all, it may turn out to be a bad theory. In which case, you have wasted time using a bad theory when you could be using another. (in some cases, this may actually be beneficial, as I will be exploring later) Others may be more optimistic and have exactly the opposite reaction: maybe its a good theory, and so we should test it! My ABM seeks to model how homogeneous and heterogeneous communities consisting of these agents fare against eachother and purely rational agents.

In my model, there are three types of character. The risk taking, the risk averse, and normal rational agent we met earlier. There is one parameter which controls all these agents' behaviour, $\lambda$. The preference ordering, which was previously just the mean of each distribution, is now weighted according to variance and the value of  $\lambda$. That is:

$$U_{T} = \hat{\mu}_T \times (1 - \lambda \times 4\sigma^{2}_T)$$

(The 4 is there just to scale the varience to lie between 0 and 1, not 0 and 0.25.)

With this setup, positive $\lambda$ makes the agents risk averse. They will weight means with high varience lower. This makes them "exploiters." Negative $\lambda$ does the opposite, and makes agents weigh means with high variance higher. This makes them "explorers" and means that they may pick theories with low means just because they are underexplored.

This produces some pretty fascinating results. Communities consisting solely of risk seeking agents tend to do better in the long run than purely rational agents, but worse in the short term. Communities consisting of risk averse agents can, in some conditions, actually do better in the short term than their purely rational rivals, but tend to do far worse in the long term. The most interesting setup of all is the combination of the two. A community consisting of a majority of risk averse (or rational) agents, plus a small minority of risk takers, basically combines the best of both worlds. The long term result is almost as good as a fully risk-taking community (thus, better than the rational agents), while mitigating some of the early drawbacks. The rick averse are essentually able to step away and "piggyback" off the results of the risk-takers. Fascinatingly, this is done at the expense of the risk takers themselves. The risk averse settle on the best theory much earlier than the risk takers.
